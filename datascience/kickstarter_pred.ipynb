{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Make necessary imports\n\n#Utils\nimport numpy as np\nimport pandas as pd\nimport random\n#Graphs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n#Models and selection\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n#I/O for model\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading Data in\ndata = pd.read_csv('../input/ks-projects-201801.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looking at basic datatypes and unique values of each column**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_and_explore(data):\n    print(\"############# PREVIEW ########################\")\n    print(data.head())\n    print(\"############# DATA TYPES ########################\")\n    print(data.info())\n    print(\"############# NO. OF UNIQUE VALS ########################\")\n    print(data.nunique())\n\nload_and_explore(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows us that the launched and deadline columns, which should be datetypes are columns and so should be converted. The state column should be converted to a boolean value, and needs to be dealt with as there should only be two states. Let's deal with this now."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[\"state\"].value_counts())\ndef convert_to_bool(item):\n    if item==\"failed\":\n        return 0\n    elif item==\"successful\":\n        return 1\n    else:\n        return 2\n\ndata[\"state\"] = data[\"state\"].apply(convert_to_bool)\nprint(data[\"state\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that 46986 columns are neither successful or failed, and are cancelced, still live, suspended or undefined. So we delete these. Now, set the id column as id, and start generating some new features. First, we will look at the date. From the launched and deadline columns, the natural features to consider are:\n1. Duration\n2. Month it was launched in\n3. Month of Deadline\n4. Quarter it was launched in\n5. Quarter of Deadline\n\nThe year doesn't make much sense to consider as we will be making predictions on live projects and so the year doesn't make sense. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delete rows that are not 0,1 in state\ndata = data[data.state !=2 ]\n\n#Set ID\ndata = data.set_index('ID')\nprint(data.head())\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new features from launched, deadline\n\n#First, convert to datetime\ndata[\"launched\"] = pd.to_datetime(data[\"launched\"], format=\"%Y-%m-%d %H:%M:%S\")\ndata[\"deadline\"] = pd.to_datetime(data[\"deadline\"], format=\"%Y-%m-%d\")\n\n#Create new features\n\n#Duration in days\ndata[\"duration\"] = (data[\"deadline\"] - data[\"launched\"]).dt.days\n\n#Quarter, month of launched and deadline date\ndata[\"launch_month\"] = data[\"launched\"].dt.month\ndata[\"launch_quarter\"] = data[\"launched\"].dt.quarter\ndata[\"deadline_month\"] = data[\"deadline\"].dt.month\ndata[\"deadline_quarter\"] = data[\"deadline\"].dt.quarter\n\n#The launch hour may also have an impact, as it may affect when/if it goes viral\ndata[\"launch_hour\"] = data[\"launched\"].dt.hour\ndata[['launched','deadline', 'duration','launch_month', 'launch_quarter', 'deadline_month', 'deadline_quarter']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create new features from the title now. Performing sentiment analysis or other NLP techniques doesn't make much sense as each title is quite different as it describes the specific project, and may have an impact on making the title vectorization the same as the category. Therefore, I will stick to basic title vectorization techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new features from title\n\n#Length of title\ndata[\"title_length\"] = data[\"name\"].apply(lambda x: len(str(x)))\n\n#Number of words\ndata[\"title_words\"] = data[\"name\"].apply(lambda x: len(str(x).split(' ')))\n\n#Number of symbols\ndata['title_symbols'] = data[\"name\"].apply(lambda x: str(x).count('!') + str(x).count('?'))\n\ndata[['title_length', 'title_words', 'title_symbols']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now finally, some additional features will be created based on the backers/pledged. Make any inferences from the pledged column is risky because of the fact that we don't know when the data was scraped, so I am not sure if considering that column heavily is a smart idea. I will only look at two metrics: amount pledged per backer."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new pledged feature\n\n#New column for if there are any backers\ndata[\"backers_exist\"] = np.where(data[\"backers\"]>0, \"True\", \"False\")\n\n#Make a mask for rows that contain backers\nmask_backers_exist = (data[\"backers\"]>0)\n\n#Enter 0 for where backers don't exist, and the pledged per backer for where they do\ndata['pledged_per_backer'] = 0\ndata.loc[mask_backers_exist, 'pledged_per_backer'] = data[\"pledged\"] / data[\"backers\"]\n\n#Review new metric\nprint(data[[\"pledged\", \"backers\", \"pledged_per_backer\"]].head())\nprint(\"MAX VALS\")\nprint(data[[\"pledged\", \"backers\",\"pledged_per_backer\"]].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"Now we must find those columns with NaN values, and deal with them accordingly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find columns with NaN values\ndata.isna().any()[lambda x:x]\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the name, usd pledged columns have NaN values. Therefore, we must deal with this on a case by case basis. Let's look at the name column's null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"name\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, since there are only 4, we can replace the features created from the title for these with the means. Let's look at the USD pledged column."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"usd pledged\"].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, we see that the columns that are null for usd pledged have a weird country value as well. We do know the currency, and so getting the country shouldn't be difficult. From this, we can replace the country column and the usd pledged column by making an exchange rate dictionary. So, let's do this. First, we find all the currencies. Then, find the country from the currency, and then the usd pledged from the currency."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get Currencies and Countries\nprint(data[\"currency\"].value_counts())\nprint(data[\"country\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make dictionary mapping currency to country code\ncurr_to_country  = {\n    \"USD\": [\"US\"],\n    \"GBP\": [\"GB\"],\n    \"EUR\": [\"DE\", \"FR\", \"IT\", \"NL\",\"ES\", \"IE\", \"BE\", \"AT\", \"LU\"], #Denmark, France, Italy, Netherlands, Spain, Ireland, Belgium, Austria, Luxembourg\n    \"CAD\": [\"CA\"],\n    \"AUD\": [\"AU\"],\n    \"SEK\": [\"SE\"],\n    \"MXN\": [\"MX\"],\n    \"NZD\": [\"NZ\"],\n    \"DKK\": [\"DK\"],\n    \"CHF\": [\"CH\"],\n    \"NOK\": [\"NO\"],\n    \"HKD\": [\"HK\"],\n    \"SGD\": [\"SG\"],\n    \"JPY\": [\"JP\"]\n}\n\n#Find all rows with bad country names\nmask_bad_countries = (data[\"country\"]=='N,0\"')\n\n#Randomly get country from the EUR array, because we can not know which country it was\ndata[\"good_country\"] = data[\"currency\"].apply(lambda x: random.choice(curr_to_country[x]))\n\n#Replace the bad countries with the fixed country. Don't do for all as that will lose truth due to the EUR\ndata.loc[mask_bad_countries, \"country\"] = data[\"good_country\"]\n\ndata[\"country\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the faulty data has been removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now find USD pledged based on currency\ncurr_usd_exchange_rate = {\n    \"USD\": 1.0,\n    \"CAD\": 0.75,\n    \"MXN\": 0.052,\n    \"SGD\": 0.73,\n    \"EUR\": 1.12,\n    \"AUD\": 0.69,\n    \"CHF\": 1.00,\n    \"DKK\": 0.15,\n    \"GBP\": 1.26,\n    \"HKD\": 0.13,\n    \"JPY\": 0.0092,\n    \"NOK\": 0.11,\n    \"NZD\": 0.65,\n    \"SEK\": 0.11\n}\n\n#Get exchange rate\ndata[\"exchange_rate\"] = data[\"currency\"].apply(lambda x: curr_usd_exchange_rate[x])\n\n#Get the good value of usd pledged\ndata[\"usd_pledged_new\"] = data[\"pledged\"] * data[\"exchange_rate\"]\n\n#Replace all bad values with the good value\ndata.loc[mask_bad_countries, \"usd pledged\"] = data[\"usd_pledged_new\"]\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, all the NaN values for the country and the usd pledged have been fixed! Now, we can just drop the NaN rows with the names, as they are only 4 rows like this."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna()\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that all of the NaN and faulty values have been dealt with, we can start filtering the important features. To start, we will do some manual analysis here. We will start by looking at the successes/fails of the date features to see if there is much variation."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to generate labels, success/fail ratios for columns\ndef gen_bar_chart_data(columns):\n    labels = []\n    ratios = []\n    for col in date_visualize_columns:\n        label = success_rows[col].value_counts().keys().tolist()\n        count_fails = fail_rows[col].value_counts().tolist()\n        count_successes = success_rows[col].value_counts().tolist()\n\n        ratio = []\n        for i in range(len(label)):\n            ratio.append(count_fails[i]/count_successes[i])\n\n        labels.append(label)\n        ratios.append(ratio)\n    \n    return labels, ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate the data - launched, launched hour, deadline, duration, launched quarter, deadline quarter\ndate_visualize_columns = [\"launch_hour\", \"duration\", \"launch_quarter\", \"deadline_quarter\"]\n\nplt.style.use('fivethirtyeight')\n\n#Fails, successes\nfail_rows = data.loc[data['state']==0]\nsuccess_rows = data.loc[data['state']==1]\n\n#Labels, ratios\nlabels, ratios = gen_bar_chart_data(date_visualize_columns)\n      \n#Plot\nfig1, (graph1, graph2) = plt.subplots(2)\nfig2, (graph3, graph4) = plt.subplots(2)\n\ngraph1.bar(labels[0], ratios[0])\ngraph1.set_title(date_visualize_columns[0])\n\ngraph2.bar(labels[1], ratios[1])\ngraph2.set_title(date_visualize_columns[1])\n\ngraph3.bar(labels[2], ratios[2])\ngraph3.set_title(date_visualize_columns[2])\n\ngraph4.bar(labels[3], ratios[3])\ngraph4.set_title(date_visualize_columns[3])\n\nfig1.suptitle(\"Date columns\")\nfig2.suptitle(\"Quarter Data\")\nfig1.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These graphs show that there is significant variation in the success/fail ratios in the launch hour and duration, but not as much in the launch and deadline quarter. Next we will look at similar graphs for the title features we generated. "},{"metadata":{"trusted":true},"cell_type":"code","source":"date_visualize_columns = [\"title_length\", \"title_words\", \"title_symbols\"]\n\nlabels, ratios = gen_bar_chart_data(date_visualize_columns)\n\nfig1, (graph1, graph2) = plt.subplots(2)\nfig2, (graph3) = plt.subplots(1)\n\ngraph1.bar(labels[0], ratios[0])\ngraph1.set_title(date_visualize_columns[0])\n\ngraph2.bar(labels[1], ratios[1])\ngraph2.set_title(date_visualize_columns[1])\nfig1.suptitle(\"Title Columns\")\n\ngraph3.bar(labels[2], ratios[2])\ngraph3.set_title(date_visualize_columns[2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, while there is no clear correlation, all of these seem to be of importance. Next, we will find correlations between variables and plot a heatmap to see where important correlations are. This will give us a preliminary overview of important relationships in our dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get top correlations\ncorr_matrix = data.corr()\ntop_corr_features = corr_matrix.index\nplt.figure(figsize=(20,20))\n\n#Plot heatmap\ngraph = sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see a good correlation between the backers and the amount pledged, which furthers the importance of creating the feature that we created. Additionally, there is a blob of high correlations around the launch/deadline month/quarter and the features of the title, but this is pretty artificial as it is simply because they point to the same thing. For this reason, we will choose not to keep the deadline/launch month, but will keep the quarter as it might still be important. Additionally, looking at the state, we see that there is a slightly higher correlation in the title features, so we will make sure not to remove those. Finally, we will use a chi squared test to find the top 10 most important features in the dataset. Before we do that, however, we must one-hot encode our dataset. First, lets see what dtypes we have. "},{"metadata":{"trusted":true},"cell_type":"code","source":"object_data = data.select_dtypes(include=['object'])\nobject_data.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the columns that are not a numerical type. We won't be using the name, backers_exist or good_country columns as features were already extracted from them (name) or were used for the understanding of other data. Therefore, we need to one hot encode the following columns. First, we will look at the values in them and see if they are appropriate for future purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_one_hot_encode = ['category', 'main_category', 'currency','country', 'launch_quarter', 'deadline_quarter', 'launch_hour']\nfor col in cols_one_hot_encode:\n    print(data[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, we see that some values in the category and main_category columns contains an ampersand (&) while some contain spaces and some contain hyphens (-). These need to be removed. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_encoded = pd.get_dummies(data, prefix=cols_one_hot_encode, columns=cols_one_hot_encode)\ndata_encoded.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we must finalize our dataframe by removing the columns that we don't want analyzed in our model. These are \n1. Name, as we have already extracted important features from it. \n2. Launch month, as we did not see any importance in our manual analysis.\n3. Deadline month, as we did not see any importance in our manual analysis.\n4. Backers Exist,\n5. Good Country,\n6. Exchange Rate, \n7. USD pledged New, as these were for our preprocessing.\n8. Launched, as it is a timestamp.\n9. Deadline, as it is a timestamp.\n10. Pledged,\n11. usd pledged,\n12. usd pledged real, as they introduce data leakage into our dataset.\n13. backers, to prevent data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_analyze = data_encoded.drop(['launched','pledged','usd pledged','backers', 'usd_pledged_real', 'deadline','name', 'launch_month', 'deadline_month', 'backers_exist', 'good_country', 'exchange_rate', 'usd_pledged_new'], axis=1)\n#We will first need the mean and standard deviation of all of our columns so that we can use it to make predictions later. \nmean_std = data_analyze.agg([np.mean, np.std])\n\n#Write to file\nmean_std.to_csv('statistics.csv')\nmean_std.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check dtypes and shape of dataframe now\ndata_analyze.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, everything is an integer float or datetime now, and so is good to go. It is now 256 columns wide and we only removed 4 rows. We will now create two arrays: one with the names of all features and one with the names of the target. Then, we will normalize the features of our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove state from independent columns\nfeatures = list(data_analyze)\nfeatures.remove('state')\n\n#Not sure if OHE columns should be normalized, so I will do it for now and then experiment later to see which gives better results\ndata_analyze_scaled = pd.DataFrame(preprocessing.normalize(data_analyze[features]))\ndata_analyze_scaled.columns = features\n\ndata_analyze_scaled.index = data_analyze.index\n\n# # #Set same index column\n# data_analyze_scaled['index'] = data_analyze_scaled.index\n# data_analyze['index'] = data_analyze.index\n\n# # #Add target to normalized dataframe\n# data_analyze_scaled = data_analyze_scaled.merge(data_analyze[['index', 'state']], left_on='index', right_on='index')\n# data_analyze_scaled = data_analyze_scaled.set_index('index')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_analyze_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the target array\ntarget = data_analyze['state']\n\n#Show final data before splitting\nprint(target[:5])\ndata_analyze_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to split this into a training and testing set. However, the dataset has an imbalance in the number of successes/fails, therefore we need to ensure that our training set does not have this imbalance as well. A maximum success/fail of 1.5 has been chosen, and we will now split our data to ensure it falls within this ratio.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loop until a good ratio < 1.5 is found\nsuccess_fail_ratio = 10\niterated = 0\n\nwhile success_fail_ratio > 1.5:\n    features_train, features_test, target_train, target_test = train_test_split(\n        data_analyze_scaled,\n        target, \n        random_state=42,\n        test_size = 0.2)\n    \n    counts = target_train.value_counts().tolist()\n    success_fail_ratio = counts[0]/counts[1]\n    iterated+=1\n\nprint(\"ITERATED %d times to get a ratio of %f\" % (iterated, success_fail_ratio))\n\n#Error check data leakage\nif 'state' in list(features_train) or 'state' in list(features_test) or 'state' in list(data_analyze_scaled_x):\n    print(\"PROBLEM\")\nelse:\n    print(\"FINE\")\n    \nprint(\"Training shape (%d, %d)\" % (features_train.shape), (target_train.shape))\nprint(\"Testing shape (%d, %d)\" % (features_test.shape), (target_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Creation "},{"metadata":{},"cell_type":"markdown","source":"Create a baseline Gaussian Bayes Classifier to get a minimum to base other models off of."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = GaussianNB()\n\n#Fit\nbase_model_fit = base_model.fit(features_train, target_train)\n\n#Predict\npred = base_model.predict(features_test)\naccuracy = classification_report(pred, target_test)\n\nprint('\\n Percentage accuracy')\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A logistic regression model is next!"},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\n\n#Fit\nlogreg.fit(features_train, target_train)\n\n#Predict\npred = logreg.predict(features_test)\naccuracy = classification_report(pred, target_test)\n\nprint('\\n Percentage accuracy')\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, LGBM Models are popular for classification tasks like this, so we will utilize them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM Classifier. Get slightly tuned parameters with plug and play testing \nlgbm_class = LGBMClassifier(\n        n_estimators=300,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=10,\n        reg_alpha=.1,\n        reg_lambda=.05,\n        min_split_gain=.005\n    )\n\nlgbm_class.fit(features_train, \n        target_train,\n        eval_set= [(features_train, target_train), (features_test, target_test)], \n        eval_metric='auc', \n        verbose=0, \n        early_stopping_rounds=30\n       )\n\npred = lgbm_class.predict(features_test)\n\nprint('\\n Percentage accuracy')\nprint(classification_report(pred, target_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, since LGBM performed the best (as expected), train it on all of the data. I won't be able to see the accuracy this time. "},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = LGBMClassifier(\n        n_estimators=300,\n        num_leaves=30,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=10,\n        reg_alpha=.1,\n        reg_lambda=.05,\n        min_split_gain=.005\n    )\n\n#Fit\nfinal_model.fit(data_analyze_scaled, \n        target,\n        eval_set= [(features_train, target_train), (features_test, target_test)], \n        eval_metric='auc', \n        verbose=0, \n        early_stopping_rounds=30\n       )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get most important features from the LGBM Model\nfeature_importances = sorted(zip(final_model.feature_importances_, features), reverse=True)\n\n#Construct dataframe to save\nfeature_importances_df = pd.DataFrame(feat_importances)\nfeature_importances_df.columns = ['importance', 'feature']\nfeature_importances_df.index = feature_importances_df.feature\nfeature_importances_df.drop('feature', 1, inplace=True)\n\n#Show dataframe\nprint(feature_importances_df.head())\n\n#Save the file\nfeature_importances_df.to_csv('feature_importances.csv')\nfeature_importances_df\n\n#Save the model both .sav and .pkl\npickle.dump(final_model, open('finalmodel.sav', 'wb'))\npickle.dump(final_model, open('finalmodel.pkl', 'wb'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we are done :) Now natural next steps include\n1. Turning this from a classification problem into a regression problem, with the % goal achieved column as the target.\n2. Using GridSearchCV to tune hyperparameters."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}