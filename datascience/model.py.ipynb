{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was severely limited by my time crunch, and I am disappointed that I can't present my best possible work to you. I was at a hackathon on Friday and Saturday nights, and came back at midnight on Sunday due to the late awards ceremony. Hence, I only had Monday night and one night the week prior to work on this. I spent the night learning everything I now know about data science and the statistical algorithms, and now have at least an elementary understanding of most of their functioning. I hope you see, even though this may not be my best work yet, that I am incredibly commmited and motivated to learn on the go, and I promise to work as hard as this, if not more, over the 4 months of the summer! \n",
    "\n",
    "This is my solution for Question 1. I started by making the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class called Prediction that will act as my predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "\tdef __init__(self, filepath):\n",
    "\t\tself.data = pandas.read_csv(filepath)\n",
    "        self.model = 0\n",
    "\t\tself.traindata = 0\n",
    "\t\tself.testdata = 0\n",
    "\t\tself.successes = self.data.loc[data['state']=='successful']\n",
    "\t\tself.fails = self.data.loc[data['state']=='failed']\n",
    "\t\tself.targetTrain = 0\n",
    "\t\tself.targetTest = 0        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to remove all rows that contain anything in Row 14, manually titled 'exists', as this implies that the data is messed up as it is out of order. I found this by first manually inspecting the data for some time. I do all of the preprocessing in the init function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tbadrows = self.data[self.data['exists'].notnull()]\n",
    "\t\tself.data = pd.concat([self.data, badrows, badrows]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the rows that don't contain a type datetime in them. in the columns launched and deadline. They have type int instead, which was found by listing all the types in the two columns. We can use iterrows() even though it is slow, as I wrote to the csv file already with the fixed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tindicesToDelete = []\n",
    "\t\tindicesToKeep = []\n",
    "\n",
    "\t\tfor index, row in data.iterrows():\n",
    "\t\t\ttry:\n",
    "\t\t\t\tint(row['deadline'])\n",
    "\t\t\t\tindicesToDelete.append(row[index])\n",
    "\t\t\texcept:\n",
    "\t\t\t\tindicesToKeep.append(index)\n",
    "\t\tself.data = self.data.drop(indicesToDelete, axis=0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datetime in deadline and launched column to datetime object that python can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef convertToTime(self, date):\n",
    "            dateupdate = datetime.strptime(date, '%Y-%m-%d %H:%M')\n",
    "            return dateupdate\n",
    "        #In the init\n",
    "        self.data = self.data['launched'].apply(convertToTime)\n",
    "\t\tself.data = self.data['deadline'].apply(convertToTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert numbers in the two number columns floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tself.data[\"goal\"] = self.data['goal'].astype('float')\n",
    "\t\tself.data[\"pledged\"] = self.data['pledged'].astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new column for time difference. This is one of the features I will be seeing if it is relevant. This is just defined as the length between the deadline and when it was launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tself.data['timeDifference'] = (self.data['deadline'] - data['launched'])\n",
    "\t\tself.data['timeDifference'] = self.data.timeDifference.dt.total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to convert my target, the state, to a boolean (however I am using 1 and 0 so that the algorithms can understand this). Then, I want to delete all entries that are not success/fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef convertToBool(self, item):\n",
    "        if item=='failed':\n",
    "            return 0\n",
    "        if item=='successful':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "        #In the init\n",
    "        self.data['state'] = self.data['state'].apply(convertToBool)\n",
    "\t\tself.data = self.data.drop(self.data[self.data.state == 2].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to play with the launched/deadline fields a little more and see how the month of the deadline and month it was launched has an impact on success/fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t#Want to convert datetime to month in new column\n",
    "\t\tself.data['month_launched'] = (self.data.launched.dt.month)\n",
    "\t\tself.data['month_deadline'] = (self.data.deadline.dt.month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am in a position to manually assess the relevance of specific features. I define genBarGraph() to generate bar graphs of success/fail ratios for a specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef genBarGraph(self, column):\n",
    "\t\t#Edit a little to generate non-ratio graphs\n",
    "\t\tlabels = self.fails[column].value_counts().keys().tolist()\n",
    "\t\tcountsfails = self.fails[column].value_counts().tolist()\n",
    "\t\tcountssuccesses = self.successes[column].value_counts().tolist()\n",
    "\n",
    "\t\tratios = []\n",
    "\t\tfor i in range(len(labels)):\n",
    "\t\t\tratios.append(countsfails[i]/countssuccesses[i])\n",
    "\n",
    "\t\tindex = np.arange(len(labels))\n",
    "\t\tplt.xticks(index, labels, fontsize=5, rotation=30)\n",
    "\t\tplt.xlabel(column, fontsize=5)\n",
    "\t\tplt.ylabel('Ratio', fontsize=5)\n",
    "\t\tplt.title('Success/Fail ratio for each' + column)\n",
    "\t\tplt.bar(index, ratios)\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\treturn 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using this function, I generated the following graphs that are shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](graphs/categoryRatio.png) ![title](graphs/countryRatio.png) ![title](graphs/deadlineMonthRatio.png) ![title](graphs/mainCategoryRatio.png) ![title](graphs/monthLaunched.png) ![title](graphs/currencyRatio.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs, I can determine that the category, country, the currency and the main category have a wide variation the success/fail ratios, therefore we will delete deadline month and launched month as they are not relevant. Instead of deleting them explicitly, I just will not encode them and them they will get filtered out later. Now, I can start encoding the discrete data. I defined a function genEncoder that uses scikit's LabelEncoder() and OneHotEncoder() libraries to create encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef genEncoder(self, data, columns):\n",
    "\t\tfor i in columns:\n",
    "\t\t\tle_column = LabelEncoder()\n",
    "\t\t\tohe_column = OneHotEncoder()\n",
    "\n",
    "\t\t\tself.data[i+'Encoded'] = le_column.fit_transform(self.data[i].fillna('0'))\n",
    "\n",
    "\t\t\tcolumnBinaryArray = ohe_column.fit_transform(self.data[i+'Encoded'].values.reshape(-1,1)).toarray()\n",
    "\t\t\tcategoryOneHot = pd.DataFrame(columnBinaryArray, columns = [i+ '_' + str(int(k)) for k in range(columnBinaryArray.shape[1])])\n",
    "\n",
    "\t\t\tself.data = pd.concat([self.data, categoryOneHot], axis=1)\n",
    "\n",
    "\t\treturn data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the init function, I can just call this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tcolumns = ['country', 'main_category','currency',  'category']\n",
    "\t\tself.data = self.genEncoder(self.data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to vectorize the title. Here, I explored a few options such as word2vec and gloVe, along with potentially doing sentiment analysis. However, my argument for not doing those is that the titles of these projects are so specific and wildly descriptive, that those two algorithms might not be the best way to proceed. Titles may contain made-up words that would never appear again, and so would make the vectorizing not very useful for comparisons. Therefore, I vectorize the titles simply through comparing their title lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tself.data['titleLength'] = self.data['name'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to scale the data. The reason why it's imperative that we do this is because some of the algorithms being used later need scaled data, and give weird results otherwise. This code block also filters out all those data fields that are not type float64 (such as month launched and month deadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\tfor i in self.data.keys():\n",
    "\t\t\tif self.data[i].dtype == 'float64' and i!='ID' and i!='state':\n",
    "\t\t\t\tself.data[i] = scale(self.data[i])             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled our data, we can start generating test data. I used the traintestsplit function from scikit. However, before I can get a dataset, I need to make sure that the train set created is balanced, this means that it has around an even set of success/fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef genTrainTestData(self):\n",
    "\t\t#Delete all unncessary, unvectorized columns\n",
    "\t\tfor i in data.keys():\n",
    "\t\t\tif not data[i].dtype =='float':\n",
    "\t\t\tdata = data.drop(i, axis=1)\n",
    "            \n",
    "\t\t#count = self.data['state'].value_counts() #Done to check how many success/fails. 1.48 ratio found.\n",
    "\t\t#It is important to check this as we need to make sure our data is balanced.\n",
    "\t\t#A ratio of 1.5 is acceptable, especially given our data and after consulting with internet resources\n",
    "\t\t\n",
    "\t\tfeatures = data.values[:, :data.columns.get_loc(\"state\")]\n",
    "\t\ttarget = data.values[:, data.columns.get_loc(\"state\")]\n",
    "\n",
    "\t\tsuccessfailratio = 10\n",
    "\n",
    "\t\twhile successfailratio > 1.5:\n",
    "\t\t\tself.featureTrain, self.featureTest, self.targetTrain, self.targetTest = train_test_split(featuers,target, test_size=0.2)\n",
    "\t\t\tcounts = self.train['state'].value_counts().tolist()\n",
    "\t\t\tsuccessfailratio = counts[0]/counts[1]\n",
    "\n",
    "\t\t#Remove the NaN values from each array. It would be better if I replace these with the means of each column instead of just 0.\n",
    "\t\tself.featureTrain = np.nan_to_num(self.featureTrain)\n",
    "\t\tself.featureTest = np.nan_to_num(self.featureTest)\n",
    "\t\tself.targetTrain = np.nan_to_num(self.targetTrain)\n",
    "\t\tself.targetTrain = np.nan_to_num(self.targetTrain)\n",
    "\n",
    "\t\treturn 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finally generated our test and training data, we can start implementing some machine learning algorithms to generate a model. I will start by implementing the Gaussian Naive-Bayes Classification algorithm. Though it assumes independence, we can use it as a base classifier for our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef gaussianNaiveBayes(self):\n",
    "\t\tmodel = GaussianNB()\n",
    "\t\tmodel.fit(self.featureTrain, self.targetTrain)\n",
    "\n",
    "\t\tprediction = model.predict(self.targetTest)\n",
    "\t\tpred_accuracy = classification_report(self.targetTest, prediction)\n",
    "    \n",
    "        self.model = model\n",
    "\t\treturn pred_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gives us a precision accuracy of 0.65, which is better than 0.5 so it is already a decent model to use.\n",
    "\n",
    "![title](results/naivebayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the function for logistic regression. Logistic regression is also not the best algorithm to use, but we will use it here as it will allow me to compare it with other algorithms. It fits our desires well as it is a binary classification algoritns, and works well with high dimensions. We also removed the date of the deadline and launch, which may have been redundant pieces of information that would make the algorithm less efficient. One thing for the future may be to see the similarities between country/currency and other similar fields as this also fogs the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef logisticRegression(self):\n",
    "\t\tmodel = LogisticRegression()\n",
    "\n",
    "\t\tmodel.fit(self.featureTrain, self.targetTrain)\n",
    "\n",
    "\t\tprediction = model.predict(self.targetTest)\n",
    "\t\tpred_accuracy = classification_report(self.targetTest, prediction)\n",
    "\n",
    "\t\treturn pred_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the results of this logistic regression, I actually ran into an error and was unable to find the solution to the bug. I didn't have much time to work on this project, and so I moved on and decided to come back to this later if I had time. Sorry! :(\n",
    "\n",
    "We can also define a function for a decision tree classification algorithm. The reason I included this is that although this is much worse than random forests as they are very prone to overfitting due to high bias, I wanted to learn how random forests work, and this seemed like a preliminary step to that. However, the results I got from this model weren't very fruitful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef decisionTreeClassifier(self):\n",
    "\t\t#Use the roc-auc scoring \n",
    "\t\tscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\t\tclf = GridSearchCV(DecisionTreeClassifier(random_state=15), param_grid={'min_samples_split': range(2, 253, 10)}, scoring=scoring, cv=5, refit='AUC', return_train_score=True)\n",
    "\t\t\n",
    "\t\tclf.fit(train, target_train)\n",
    "\t\toptimalSplit = clf.best_params_\n",
    "\n",
    "\t\t#Gives an optimalSplit of 242\n",
    "\n",
    "\t\tmodel = DecisionTreeClassifier(random_state=clf.best_params_)\n",
    "\t\tmodel.fit(self.featureTrain, self.targetTrain)\n",
    "\n",
    "\t\tprediction = model.predict(self.targetTest)\n",
    "\t\tpred_accuracy = classification_report(self.targetTest, prediction)\n",
    "\n",
    "\t\treturn pred_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/decisiontree.png)\n",
    "\n",
    "This shows that this decision tree is an almost perfect fit of the data. This might be a classic case of overfitting that decision trees are prone to. Given scikit's limitations, post-pruning is a possible, but difficult possibility. Given my restrictions due to time to learn and implement a solution, it is a better idea right now to change the original tree by restricting the max depth and the max leaf nodes, to prevent overfitting. After fiddling with the parameters a little bit, I found setting the max leaf nodes to 8 and the min sample leafs to 5, along with a min samples split of just 7, as compared to 242 as suggested by GridSearchCV, which was definitely contributing to the overfitting. This is the updated classification chart:\n",
    "\n",
    "![title](results/decisiontreeUpdated.png)\n",
    "\n",
    "We also need to define a function called predict to actually predict new data. If the input doesn't have certain fields, we fill it with the mean of that field as follows. The Decision Tree Classifier was the best model we have so far considering precision and recall as well, and so that was chosen as our current model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-3ae8d1f61b12>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3ae8d1f61b12>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    ynew = self.model.predict(data)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def Predict(self, newProject):\n",
    "    for i in self.data.keys():\n",
    "        if not i in newProject.keys():\n",
    "            newProject[i] = self.data[i].mean()\n",
    "            \n",
    "    ynew = self.model.predict(data)\n",
    "    print(\"X=%s, Predicted=%s\" % (data, ynew))\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal situation, I would also move on to other techniques for binary classification such as SVMs, random forests, and would also look more into feature engineering and the choosing of hyperparameters. This can be done by graphing learning curves amongst other techniques. However, I am currently extremely burdened by school and by my own entreprenial ventures and hackathons that I unfortunately do not have all the time to complete this. I still hope you can see how much effort I have put into this, and thinking about my skill level can appreciate my learning over the past week. Looking forward to hearing from you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
